{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    \n",
    "    tf = {}\n",
    "    df = {}\n",
    "    f = {}\n",
    "    \n",
    "    def __init__(self, k, b, corpus=None):\n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        self.corpus = None\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \n",
    "        f = {}\n",
    "        tf = {}\n",
    "        idf = {}\n",
    "        df = {}\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        \n",
    "        corpus_size = len(corpus)\n",
    "        \n",
    "        tot_doc_size = 0\n",
    "        \n",
    "        doc_id = 0\n",
    "        \n",
    "        for document in corpus:\n",
    "            tot_doc_size += len(document)\n",
    "            \n",
    "            f[doc_id] = {}\n",
    "            \n",
    "            for term in document:\n",
    "                \n",
    "                if term not in f[doc_id]:\n",
    "                    f[doc_id][term] = 1\n",
    "                else:\n",
    "                    f[doc_id][term] += 1\n",
    "                \n",
    "                if term not in tf:\n",
    "                    tf[term] = 1\n",
    "                else:\n",
    "                    tf[term] += 1\n",
    "            \n",
    "            doc_id += 1\n",
    "        \n",
    "        doc_id = 0\n",
    "        \n",
    "        for document in f:\n",
    "            for term in f[doc_id]:\n",
    "                \n",
    "                if term not in df:\n",
    "                    df[term] = 1\n",
    "                else:\n",
    "                    df[term] += 1\n",
    "            \n",
    "            doc_id += 1\n",
    "        \n",
    "        for term in tf:\n",
    "            idf[term] = math.log(1 + (corpus_size - df[term] + .5) / (df[term] + .5) )\n",
    "        \n",
    "        self.f = f\n",
    "        self.tf = tf\n",
    "        self.idf = idf\n",
    "        \n",
    "        self.avg_doc_size = tot_doc_size / corpus_size\n",
    "        self.corpus_size = corpus_size\n",
    "    \n",
    "    def search(self, query):\n",
    "        \n",
    "        score = {}\n",
    "        \n",
    "        doc_id = 0\n",
    "        \n",
    "        for document in self.corpus:\n",
    "            score[doc_id] = 0\n",
    "            for q in query:\n",
    "                \n",
    "                # Higher the inverse document frequency of the term q, higher is the importance\n",
    "                # Higher the count of term q in this document, higher the importance\n",
    "                # Larger the document is from the average document size, it is less likely to be significant\n",
    "                curr_score = self.idf.get(q, 0) * ( self.f[doc_id].get(q, 0) * (self.k + 1) ) / ( self.f[doc_id].get(q, 0) + self.k * ( 1 - self.b + (self.b * (len(self.corpus[doc_id]) / self.avg_doc_size) ) ) )\n",
    "                \n",
    "                score[doc_id] += curr_score\n",
    "            \n",
    "            doc_id += 1\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['topic',\n",
       "  'modeling',\n",
       "  'aim',\n",
       "  'find',\n",
       "  'topic',\n",
       "  'cluster',\n",
       "  'inside',\n",
       "  'corpus',\n",
       "  'text',\n",
       "  'like',\n",
       "  'mail',\n",
       "  'news',\n",
       "  'article',\n",
       "  'without',\n",
       "  'knowing',\n",
       "  'topic',\n",
       "  'first'],\n",
       " ['topic',\n",
       "  'modeling',\n",
       "  'kind',\n",
       "  'probabilistic',\n",
       "  'generative',\n",
       "  'model',\n",
       "  'used',\n",
       "  'widely',\n",
       "  'field',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'specific',\n",
       "  'focus',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'information',\n",
       "  'retrieval',\n",
       "  'recent',\n",
       "  'year',\n",
       "  'aim',\n",
       "  'topic',\n",
       "  'modeling',\n",
       "  'discover',\n",
       "  'theme',\n",
       "  'run',\n",
       "  'corpus',\n",
       "  'analyzing',\n",
       "  'word',\n",
       "  'original',\n",
       "  'text',\n",
       "  'call',\n",
       "  'theme'],\n",
       " ['machine',\n",
       "  'learning',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'topic',\n",
       "  'model',\n",
       "  'type',\n",
       "  'statistical',\n",
       "  'model',\n",
       "  'discovering',\n",
       "  'abstract',\n",
       "  'topic',\n",
       "  'occur',\n",
       "  'collection',\n",
       "  'document',\n",
       "  'topic',\n",
       "  'modeling',\n",
       "  'frequently',\n",
       "  'used',\n",
       "  'tool',\n",
       "  'discovery',\n",
       "  'hidden',\n",
       "  'semantic',\n",
       "  'structure',\n",
       "  'text',\n",
       "  'body',\n",
       "  'intuitively',\n",
       "  'given',\n",
       "  'document',\n",
       "  'particular',\n",
       "  'topic',\n",
       "  'one',\n",
       "  'would',\n",
       "  'expect',\n",
       "  'particular',\n",
       "  'word',\n",
       "  'appear',\n",
       "  'document',\n",
       "  'le',\n",
       "  'frequently',\n",
       "  'dog',\n",
       "  'bone',\n",
       "  'appear',\n",
       "  'often',\n",
       "  'document',\n",
       "  'dog',\n",
       "  'cat',\n",
       "  'meow',\n",
       "  'appear',\n",
       "  'document',\n",
       "  'cat',\n",
       "  'appear',\n",
       "  'approximately',\n",
       "  'equally',\n",
       "  'document',\n",
       "  'typically',\n",
       "  'concern',\n",
       "  'multiple',\n",
       "  'topic',\n",
       "  'different',\n",
       "  'proportion',\n",
       "  'thus',\n",
       "  'document',\n",
       "  'cat',\n",
       "  'dog',\n",
       "  'would',\n",
       "  'probably',\n",
       "  'time',\n",
       "  'dog',\n",
       "  'word',\n",
       "  'cat',\n",
       "  'word',\n",
       "  'topic',\n",
       "  'produced',\n",
       "  'topic',\n",
       "  'modeling',\n",
       "  'technique',\n",
       "  'cluster',\n",
       "  'similar',\n",
       "  'word',\n",
       "  'topic',\n",
       "  'model',\n",
       "  'capture',\n",
       "  'intuition',\n",
       "  'mathematical',\n",
       "  'framework',\n",
       "  'allows',\n",
       "  'examining',\n",
       "  'set',\n",
       "  'document',\n",
       "  'discovering',\n",
       "  'based',\n",
       "  'statistic',\n",
       "  'word',\n",
       "  'topic',\n",
       "  'might',\n",
       "  'document',\n",
       "  'balance',\n",
       "  'topic'],\n",
       " ['latent',\n",
       "  'topic',\n",
       "  'hidden',\n",
       "  'bunch',\n",
       "  'text',\n",
       "  'want',\n",
       "  'algorithm',\n",
       "  'put',\n",
       "  'cluster',\n",
       "  'make',\n",
       "  'sense',\n",
       "  'u'],\n",
       " ['approach',\n",
       "  'temporal',\n",
       "  'information',\n",
       "  'include',\n",
       "  'block',\n",
       "  'newman',\n",
       "  'determination',\n",
       "  'temporal',\n",
       "  'dynamic',\n",
       "  'topic',\n",
       "  'pennsylvania',\n",
       "  'gazette']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Topic Modeling aims to find the topics (or clusters) inside a corpus of texts (like mails or news articles), without knowing those topics at first. ',\n",
    "    'Topic Modeling is a kind of a probabilistic generative model that has been used widely in the field of computer science with a specific focus on text mining and information retrieval in recent years. The aim of topic modeling is to discover the themes that run through a corpus by analyzing the words of the original texts. We call these themes \\“topics.\\” ',\n",
    "    'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\\'s balance of topics is.',\n",
    "    'Latent because the topics are “hidden”. We have a bunch of texts and we want the algorithm to put them into clusters that will make sense to us.',\n",
    "    'Approaches for temporal information include Block and Newman\\'s determination of the temporal dynamics of topics in the Pennsylvania Gazette during 1728–1800.'\n",
    "]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# remove all tokens that are not alphabetic\n",
    "preprocessed = [[word.lower() for word in word_tokenize(document) if word.isalpha()] for document in corpus]\n",
    "\n",
    "# remove stop words\n",
    "preprocessed = [[lemmatizer.lemmatize(word) for word in document if word not in stop_words] for document in preprocessed]\n",
    "\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25(k=1.5, b=0.75)\n",
    "bm25.fit(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'topic modeling aim'\n",
    "query = [word.lower() for word in word_tokenize(query) if word.isalpha() not in stop_words]\n",
    "\n",
    "scores = bm25.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Modeling aims to find the topics (or clusters) inside a corpus of texts (like mails or news articles), without knowing those topics at first. -----> 2.0\n",
      "Topic Modeling is a kind of a probabilistic generative model that has been used widely in the field of computer science with a specific focus on text mining and information retrieval in recent years. The aim of topic modeling is to discover the themes that run through a corpus by analyzing the words of the original texts. We call these themes \\“topics.\\” -----> 1.823\n",
      "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.-----> 0.639\n",
      "Latent because the topics are “hidden”. We have a bunch of texts and we want the algorithm to put them into clusters that will make sense to us.-----> 0.123\n",
      "Approaches for temporal information include Block and Newman's determination of the temporal dynamics of topics in the Pennsylvania Gazette during 1728–1800.-----> 0.123\n"
     ]
    }
   ],
   "source": [
    "for score, document in zip(scores.values(), corpus):\n",
    "    score = round(score, 3)\n",
    "    print(document + '-----> ' + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
